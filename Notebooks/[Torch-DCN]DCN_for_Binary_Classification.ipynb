{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "assert torch.__version__>='1.2.0', 'Expect PyTorch>=1.2.0 but get {}'.format(torch.__version__)\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "imp_dir = '../Implementations'\n",
    "sys.path.insert(1, imp_dir)\n",
    "data_dir = '../Data/criteo'\n",
    "sys.path.insert(1, data_dir)\n",
    "\n",
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging)\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, format='%(asctime)s %(levelname)-6s %(message)s', level=logging.INFO, datefmt='%H:%M:%S')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "fh = logging.FileHandler('DCN_notebook.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(logging.Formatter('%(asctime)s %(levelname)-6s %(message)s'))\n",
    "\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:02:53 INFO   Device in Use: cuda\n",
      "06:02:53 INFO   CUDA Memory: Total 11.17 GB, Cached 0.00 GB, Allocated 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info('Device in Use: {}'.format(DEVICE))\n",
    "torch.cuda.empty_cache()\n",
    "t = torch.cuda.get_device_properties(DEVICE).total_memory/1024**3\n",
    "c = torch.cuda.memory_cached(DEVICE)/1024**3\n",
    "a = torch.cuda.memory_allocated(DEVICE)/1024**3\n",
    "logger.info('CUDA Memory: Total {:.2f} GB, Cached {:.2f} GB, Allocated {:.2f} GB'.format(t,c,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map_dict_pkl_path = os.path.join(data_dir, 'criteo_feature_dict_artifact/categorical_feature_map_dict.pkl')\n",
    "with open(embedding_map_dict_pkl_path, 'rb') as f:\n",
    "    embedding_map_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all available files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_artifact_dir = os.path.join(data_dir, 'criteo_train_numpy_artifact')\n",
    "index_artifact = sorted(list(filter(lambda x: x.split('-')[1]=='index', os.listdir(np_artifact_dir))), key = lambda x: int(x.split('.')[0].split('-')[-1]))\n",
    "value_artifact = sorted(list(filter(lambda x: x.split('-')[1]=='value', os.listdir(np_artifact_dir))), key = lambda x: int(x.split('.')[0].split('-')[-1]))\n",
    "label_artifact = sorted(list(filter(lambda x: x.split('-')[1]=='label', os.listdir(np_artifact_dir))), key = lambda x: int(x.split('.')[0].split('-')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:04:17 INFO   Training data loaded after 78.85s\n",
      "06:04:27 INFO   Test data loaded after 9.83s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_data = (\n",
    "    np.vstack([np.load(os.path.join(np_artifact_dir, f)) for f in index_artifact[:10]]),\n",
    "    np.vstack([np.load(os.path.join(np_artifact_dir, f)) for f in value_artifact[:10]]),\n",
    "    np.vstack([np.load(os.path.join(np_artifact_dir, f)) for f in label_artifact[:10]]),\n",
    ")\n",
    "\n",
    "logger.info('Training data loaded after {:.2f}s'.format(time.time()-start))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "test_data = (\n",
    "    np.vstack([np.load(os.path.join(np_artifact_dir, f)) for f in index_artifact[10:]]),\n",
    "    np.vstack([np.load(os.path.join(np_artifact_dir, f)) for f in value_artifact[10:]]),\n",
    "    np.vstack([np.load(os.path.join(np_artifact_dir, f)) for f in label_artifact[10:]]),\n",
    ")\n",
    "\n",
    "logger.info('Test data loaded after {:.2f}s'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'DCN_BinClf_Torch' from '../Implementations/DCN_BinClf_Torch.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import execution\n",
    "importlib.reload(execution)\n",
    "import DCN_BinClf_Torch \n",
    "importlib.reload(DCN_BinClf_Torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCN = DCN_BinClf_Torch.DCN_Layer(len(embedding_map_dict)+60,\n",
    "                                 20,\n",
    "                                 26,\n",
    "                                 13,\n",
    "                                 [1024 for _ in range(2)],\n",
    "                                 [0.5 for _ in range(3)],\n",
    "                                 6).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "checkpoint_dir = os.path.join(cwd, 'DCN_artifact')\n",
    "checkpoint_prefix = 'DCN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:20:20 INFO   Epoch 1/5 - Batch 1000/64000 Done - Train Loss: 0.493846, Val Loss: 0.467414\n",
      "01:24:20 INFO   Epoch 1/5 - Batch 2000/64000 Done - Train Loss: 0.482650, Val Loss: 0.459085\n",
      "01:28:20 INFO   Epoch 1/5 - Batch 3000/64000 Done - Train Loss: 0.477697, Val Loss: 0.502537\n",
      "01:32:20 INFO   Epoch 1/5 - Batch 4000/64000 Done - Train Loss: 0.474821, Val Loss: 0.428925\n",
      "01:36:21 INFO   Epoch 1/5 - Batch 5000/64000 Done - Train Loss: 0.472362, Val Loss: 0.468349\n",
      "01:40:21 INFO   Epoch 1/5 - Batch 6000/64000 Done - Train Loss: 0.470290, Val Loss: 0.453534\n",
      "01:44:21 INFO   Epoch 1/5 - Batch 7000/64000 Done - Train Loss: 0.468951, Val Loss: 0.478359\n",
      "01:48:21 INFO   Epoch 1/5 - Batch 8000/64000 Done - Train Loss: 0.467425, Val Loss: 0.444895\n",
      "01:52:21 INFO   Epoch 1/5 - Batch 9000/64000 Done - Train Loss: 0.466191, Val Loss: 0.472482\n",
      "01:56:21 INFO   Epoch 1/5 - Batch 10000/64000 Done - Train Loss: 0.465137, Val Loss: 0.466647\n",
      "02:00:22 INFO   Epoch 1/5 - Batch 11000/64000 Done - Train Loss: 0.464125, Val Loss: 0.460116\n",
      "02:04:22 INFO   Epoch 1/5 - Batch 12000/64000 Done - Train Loss: 0.463252, Val Loss: 0.484666\n",
      "02:08:22 INFO   Epoch 1/5 - Batch 13000/64000 Done - Train Loss: 0.462583, Val Loss: 0.426944\n",
      "02:12:22 INFO   Epoch 1/5 - Batch 14000/64000 Done - Train Loss: 0.462520, Val Loss: 0.437463\n",
      "02:16:22 INFO   Epoch 1/5 - Batch 15000/64000 Done - Train Loss: 0.462472, Val Loss: 0.468400\n",
      "02:20:22 INFO   Epoch 1/5 - Batch 16000/64000 Done - Train Loss: 0.462297, Val Loss: 0.448818\n",
      "02:24:22 INFO   Epoch 1/5 - Batch 17000/64000 Done - Train Loss: 0.462177, Val Loss: 0.475588\n",
      "02:28:22 INFO   Epoch 1/5 - Batch 18000/64000 Done - Train Loss: 0.461965, Val Loss: 0.470784\n",
      "02:32:22 INFO   Epoch 1/5 - Batch 19000/64000 Done - Train Loss: 0.461871, Val Loss: 0.464876\n",
      "02:36:22 INFO   Epoch 1/5 - Batch 20000/64000 Done - Train Loss: 0.461737, Val Loss: 0.490847\n",
      "02:40:21 INFO   Epoch 1/5 - Batch 21000/64000 Done - Train Loss: 0.461624, Val Loss: 0.432727\n",
      "02:44:21 INFO   Epoch 1/5 - Batch 22000/64000 Done - Train Loss: 0.461454, Val Loss: 0.509777\n",
      "02:48:21 INFO   Epoch 1/5 - Batch 23000/64000 Done - Train Loss: 0.461320, Val Loss: 0.475761\n",
      "02:52:21 INFO   Epoch 1/5 - Batch 24000/64000 Done - Train Loss: 0.461189, Val Loss: 0.455504\n",
      "02:56:21 INFO   Epoch 1/5 - Batch 25000/64000 Done - Train Loss: 0.461079, Val Loss: 0.463509\n",
      "03:00:22 INFO   Epoch 1/5 - Batch 26000/64000 Done - Train Loss: 0.460990, Val Loss: 0.445072\n",
      "03:16:21 INFO   Epoch 1/5 - Batch 30000/64000 Done - Train Loss: 0.460726, Val Loss: 0.443203\n",
      "03:20:21 INFO   Epoch 1/5 - Batch 31000/64000 Done - Train Loss: 0.460698, Val Loss: 0.465524\n",
      "03:24:21 INFO   Epoch 1/5 - Batch 32000/64000 Done - Train Loss: 0.460629, Val Loss: 0.409545\n",
      "03:28:21 INFO   Epoch 1/5 - Batch 33000/64000 Done - Train Loss: 0.460480, Val Loss: 0.475451\n",
      "03:32:20 INFO   Epoch 1/5 - Batch 34000/64000 Done - Train Loss: 0.460316, Val Loss: 0.456998\n",
      "03:36:20 INFO   Epoch 1/5 - Batch 35000/64000 Done - Train Loss: 0.460161, Val Loss: 0.472921\n",
      "03:40:20 INFO   Epoch 1/5 - Batch 36000/64000 Done - Train Loss: 0.459979, Val Loss: 0.429074\n",
      "03:44:20 INFO   Epoch 1/5 - Batch 37000/64000 Done - Train Loss: 0.459878, Val Loss: 0.449334\n",
      "03:48:19 INFO   Epoch 1/5 - Batch 38000/64000 Done - Train Loss: 0.459717, Val Loss: 0.452587\n",
      "03:52:19 INFO   Epoch 1/5 - Batch 39000/64000 Done - Train Loss: 0.459629, Val Loss: 0.410658\n",
      "03:56:19 INFO   Epoch 1/5 - Batch 40000/64000 Done - Train Loss: 0.459529, Val Loss: 0.482615\n",
      "04:00:19 INFO   Epoch 1/5 - Batch 41000/64000 Done - Train Loss: 0.459428, Val Loss: 0.457854\n",
      "04:04:18 INFO   Epoch 1/5 - Batch 42000/64000 Done - Train Loss: 0.459302, Val Loss: 0.448095\n",
      "04:08:18 INFO   Epoch 1/5 - Batch 43000/64000 Done - Train Loss: 0.459156, Val Loss: 0.457187\n",
      "04:12:18 INFO   Epoch 1/5 - Batch 44000/64000 Done - Train Loss: 0.459036, Val Loss: 0.466998\n",
      "04:16:18 INFO   Epoch 1/5 - Batch 45000/64000 Done - Train Loss: 0.458926, Val Loss: 0.437895\n",
      "04:20:17 INFO   Epoch 1/5 - Batch 46000/64000 Done - Train Loss: 0.458801, Val Loss: 0.401827\n",
      "04:24:17 INFO   Epoch 1/5 - Batch 47000/64000 Done - Train Loss: 0.458658, Val Loss: 0.466762\n",
      "04:28:17 INFO   Epoch 1/5 - Batch 48000/64000 Done - Train Loss: 0.458524, Val Loss: 0.469100\n",
      "04:32:17 INFO   Epoch 1/5 - Batch 49000/64000 Done - Train Loss: 0.458385, Val Loss: 0.472237\n",
      "04:36:16 INFO   Epoch 1/5 - Batch 50000/64000 Done - Train Loss: 0.458230, Val Loss: 0.502021\n",
      "04:40:16 INFO   Epoch 1/5 - Batch 51000/64000 Done - Train Loss: 0.458099, Val Loss: 0.449258\n",
      "04:44:16 INFO   Epoch 1/5 - Batch 52000/64000 Done - Train Loss: 0.458105, Val Loss: 0.384913\n",
      "04:48:16 INFO   Epoch 1/5 - Batch 53000/64000 Done - Train Loss: 0.458106, Val Loss: 0.432896\n",
      "04:52:16 INFO   Epoch 1/5 - Batch 54000/64000 Done - Train Loss: 0.458085, Val Loss: 0.415688\n",
      "04:56:15 INFO   Epoch 1/5 - Batch 55000/64000 Done - Train Loss: 0.458095, Val Loss: 0.419356\n",
      "05:00:15 INFO   Epoch 1/5 - Batch 56000/64000 Done - Train Loss: 0.458074, Val Loss: 0.504453\n",
      "05:04:15 INFO   Epoch 1/5 - Batch 57000/64000 Done - Train Loss: 0.458045, Val Loss: 0.403335\n",
      "05:08:15 INFO   Epoch 1/5 - Batch 58000/64000 Done - Train Loss: 0.458008, Val Loss: 0.402194\n",
      "05:12:15 INFO   Epoch 1/5 - Batch 59000/64000 Done - Train Loss: 0.457958, Val Loss: 0.417643\n",
      "05:16:14 INFO   Epoch 1/5 - Batch 60000/64000 Done - Train Loss: 0.457900, Val Loss: 0.438480\n",
      "05:20:14 INFO   Epoch 1/5 - Batch 61000/64000 Done - Train Loss: 0.457848, Val Loss: 0.459245\n",
      "05:24:14 INFO   Epoch 1/5 - Batch 62000/64000 Done - Train Loss: 0.457805, Val Loss: 0.437277\n",
      "05:28:14 INFO   Epoch 1/5 - Batch 63000/64000 Done - Train Loss: 0.457736, Val Loss: 0.460168\n",
      "06:53:40 INFO   Epoch 2/5 - Batch 1000/64000 Done - Train Loss: 0.450758, Val Loss: 0.485146\n",
      "06:57:40 INFO   Epoch 2/5 - Batch 2000/64000 Done - Train Loss: 0.449565, Val Loss: 0.430998\n",
      "07:01:39 INFO   Epoch 2/5 - Batch 3000/64000 Done - Train Loss: 0.448950, Val Loss: 0.429977\n",
      "07:05:39 INFO   Epoch 2/5 - Batch 4000/64000 Done - Train Loss: 0.448936, Val Loss: 0.452577\n",
      "07:09:38 INFO   Epoch 2/5 - Batch 5000/64000 Done - Train Loss: 0.448464, Val Loss: 0.470185\n",
      "07:13:38 INFO   Epoch 2/5 - Batch 6000/64000 Done - Train Loss: 0.448076, Val Loss: 0.474508\n",
      "07:17:37 INFO   Epoch 2/5 - Batch 7000/64000 Done - Train Loss: 0.447928, Val Loss: 0.415720\n",
      "07:21:37 INFO   Epoch 2/5 - Batch 8000/64000 Done - Train Loss: 0.447360, Val Loss: 0.467630\n",
      "07:25:37 INFO   Epoch 2/5 - Batch 9000/64000 Done - Train Loss: 0.447039, Val Loss: 0.466023\n",
      "07:29:36 INFO   Epoch 2/5 - Batch 10000/64000 Done - Train Loss: 0.446751, Val Loss: 0.428616\n",
      "07:33:36 INFO   Epoch 2/5 - Batch 11000/64000 Done - Train Loss: 0.446438, Val Loss: 0.440619\n",
      "07:37:35 INFO   Epoch 2/5 - Batch 12000/64000 Done - Train Loss: 0.446150, Val Loss: 0.423713\n",
      "07:41:35 INFO   Epoch 2/5 - Batch 13000/64000 Done - Train Loss: 0.445996, Val Loss: 0.406627\n",
      "07:45:35 INFO   Epoch 2/5 - Batch 14000/64000 Done - Train Loss: 0.446322, Val Loss: 0.457112\n",
      "07:49:34 INFO   Epoch 2/5 - Batch 15000/64000 Done - Train Loss: 0.446694, Val Loss: 0.429708\n",
      "07:53:34 INFO   Epoch 2/5 - Batch 16000/64000 Done - Train Loss: 0.446903, Val Loss: 0.464541\n",
      "07:57:34 INFO   Epoch 2/5 - Batch 17000/64000 Done - Train Loss: 0.447147, Val Loss: 0.482754\n",
      "08:01:35 INFO   Epoch 2/5 - Batch 18000/64000 Done - Train Loss: 0.447278, Val Loss: 0.463108\n",
      "08:05:35 INFO   Epoch 2/5 - Batch 19000/64000 Done - Train Loss: 0.447489, Val Loss: 0.441151\n",
      "08:09:34 INFO   Epoch 2/5 - Batch 20000/64000 Done - Train Loss: 0.447626, Val Loss: 0.443356\n",
      "08:13:34 INFO   Epoch 2/5 - Batch 21000/64000 Done - Train Loss: 0.447782, Val Loss: 0.492086\n",
      "08:17:33 INFO   Epoch 2/5 - Batch 22000/64000 Done - Train Loss: 0.447865, Val Loss: 0.407412\n",
      "08:21:32 INFO   Epoch 2/5 - Batch 23000/64000 Done - Train Loss: 0.447968, Val Loss: 0.434674\n",
      "08:25:31 INFO   Epoch 2/5 - Batch 24000/64000 Done - Train Loss: 0.448085, Val Loss: 0.425928\n",
      "08:29:30 INFO   Epoch 2/5 - Batch 25000/64000 Done - Train Loss: 0.448196, Val Loss: 0.448295\n",
      "08:33:30 INFO   Epoch 2/5 - Batch 26000/64000 Done - Train Loss: 0.448305, Val Loss: 0.437306\n",
      "08:37:30 INFO   Epoch 2/5 - Batch 27000/64000 Done - Train Loss: 0.448431, Val Loss: 0.465107\n",
      "08:41:29 INFO   Epoch 2/5 - Batch 28000/64000 Done - Train Loss: 0.448530, Val Loss: 0.452424\n",
      "08:45:29 INFO   Epoch 2/5 - Batch 29000/64000 Done - Train Loss: 0.448659, Val Loss: 0.476161\n",
      "08:49:28 INFO   Epoch 2/5 - Batch 30000/64000 Done - Train Loss: 0.448752, Val Loss: 0.489723\n",
      "08:53:28 INFO   Epoch 2/5 - Batch 31000/64000 Done - Train Loss: 0.448892, Val Loss: 0.446429\n",
      "08:57:28 INFO   Epoch 2/5 - Batch 32000/64000 Done - Train Loss: 0.448982, Val Loss: 0.468434\n",
      "09:01:27 INFO   Epoch 2/5 - Batch 33000/64000 Done - Train Loss: 0.448974, Val Loss: 0.415027\n",
      "09:05:27 INFO   Epoch 2/5 - Batch 34000/64000 Done - Train Loss: 0.448956, Val Loss: 0.392411\n",
      "09:09:27 INFO   Epoch 2/5 - Batch 35000/64000 Done - Train Loss: 0.448942, Val Loss: 0.496393\n",
      "09:13:26 INFO   Epoch 2/5 - Batch 36000/64000 Done - Train Loss: 0.448898, Val Loss: 0.498121\n",
      "09:17:26 INFO   Epoch 2/5 - Batch 37000/64000 Done - Train Loss: 0.448929, Val Loss: 0.424034\n",
      "09:21:26 INFO   Epoch 2/5 - Batch 38000/64000 Done - Train Loss: 0.448902, Val Loss: 0.477742\n",
      "09:25:25 INFO   Epoch 2/5 - Batch 39000/64000 Done - Train Loss: 0.448919, Val Loss: 0.421168\n",
      "09:29:25 INFO   Epoch 2/5 - Batch 40000/64000 Done - Train Loss: 0.448904, Val Loss: 0.419448\n",
      "09:33:24 INFO   Epoch 2/5 - Batch 41000/64000 Done - Train Loss: 0.448886, Val Loss: 0.449043\n",
      "09:37:24 INFO   Epoch 2/5 - Batch 42000/64000 Done - Train Loss: 0.448848, Val Loss: 0.439417\n",
      "09:41:23 INFO   Epoch 2/5 - Batch 43000/64000 Done - Train Loss: 0.448793, Val Loss: 0.427555\n",
      "09:45:23 INFO   Epoch 2/5 - Batch 44000/64000 Done - Train Loss: 0.448768, Val Loss: 0.485959\n",
      "09:49:22 INFO   Epoch 2/5 - Batch 45000/64000 Done - Train Loss: 0.448748, Val Loss: 0.473761\n",
      "09:53:22 INFO   Epoch 2/5 - Batch 46000/64000 Done - Train Loss: 0.448705, Val Loss: 0.445634\n",
      "09:57:21 INFO   Epoch 2/5 - Batch 47000/64000 Done - Train Loss: 0.448638, Val Loss: 0.428731\n",
      "10:01:21 INFO   Epoch 2/5 - Batch 48000/64000 Done - Train Loss: 0.448590, Val Loss: 0.424922\n",
      "10:05:20 INFO   Epoch 2/5 - Batch 49000/64000 Done - Train Loss: 0.448534, Val Loss: 0.466944\n",
      "10:09:20 INFO   Epoch 2/5 - Batch 50000/64000 Done - Train Loss: 0.448456, Val Loss: 0.428206\n",
      "10:13:19 INFO   Epoch 2/5 - Batch 51000/64000 Done - Train Loss: 0.448405, Val Loss: 0.458694\n",
      "10:17:19 INFO   Epoch 2/5 - Batch 52000/64000 Done - Train Loss: 0.448472, Val Loss: 0.451836\n",
      "10:21:18 INFO   Epoch 2/5 - Batch 53000/64000 Done - Train Loss: 0.448527, Val Loss: 0.438395\n",
      "10:25:18 INFO   Epoch 2/5 - Batch 54000/64000 Done - Train Loss: 0.448570, Val Loss: 0.476587\n",
      "10:29:17 INFO   Epoch 2/5 - Batch 55000/64000 Done - Train Loss: 0.448644, Val Loss: 0.467037\n",
      "10:33:17 INFO   Epoch 2/5 - Batch 56000/64000 Done - Train Loss: 0.448683, Val Loss: 0.442206\n",
      "10:37:17 INFO   Epoch 2/5 - Batch 57000/64000 Done - Train Loss: 0.448720, Val Loss: 0.409313\n",
      "10:41:16 INFO   Epoch 2/5 - Batch 58000/64000 Done - Train Loss: 0.448745, Val Loss: 0.446760\n",
      "10:45:16 INFO   Epoch 2/5 - Batch 59000/64000 Done - Train Loss: 0.448756, Val Loss: 0.432486\n",
      "10:49:15 INFO   Epoch 2/5 - Batch 60000/64000 Done - Train Loss: 0.448758, Val Loss: 0.443217\n",
      "10:53:15 INFO   Epoch 2/5 - Batch 61000/64000 Done - Train Loss: 0.448761, Val Loss: 0.438232\n",
      "10:57:14 INFO   Epoch 2/5 - Batch 62000/64000 Done - Train Loss: 0.448781, Val Loss: 0.466874\n",
      "11:01:14 INFO   Epoch 2/5 - Batch 63000/64000 Done - Train Loss: 0.448770, Val Loss: 0.435079\n",
      "12:23:24 INFO   Epoch 2/5 - Train Loss: 0.448761, Test Loss: 0.723258, Test ROC Score: 0.804345\n",
      "12:27:23 INFO   Epoch 3/5 - Batch 1000/64000 Done - Train Loss: 0.443876, Val Loss: 0.460980\n",
      "12:31:23 INFO   Epoch 3/5 - Batch 2000/64000 Done - Train Loss: 0.442523, Val Loss: 0.473919\n",
      "12:35:22 INFO   Epoch 3/5 - Batch 3000/64000 Done - Train Loss: 0.441678, Val Loss: 0.454709\n",
      "12:39:21 INFO   Epoch 3/5 - Batch 4000/64000 Done - Train Loss: 0.441706, Val Loss: 0.430317\n",
      "12:43:21 INFO   Epoch 3/5 - Batch 5000/64000 Done - Train Loss: 0.441425, Val Loss: 0.422447\n",
      "12:47:20 INFO   Epoch 3/5 - Batch 6000/64000 Done - Train Loss: 0.441124, Val Loss: 0.442482\n",
      "12:51:20 INFO   Epoch 3/5 - Batch 7000/64000 Done - Train Loss: 0.440925, Val Loss: 0.415783\n",
      "12:55:19 INFO   Epoch 3/5 - Batch 8000/64000 Done - Train Loss: 0.440185, Val Loss: 0.452954\n",
      "12:59:18 INFO   Epoch 3/5 - Batch 9000/64000 Done - Train Loss: 0.439853, Val Loss: 0.429003\n",
      "13:03:17 INFO   Epoch 3/5 - Batch 10000/64000 Done - Train Loss: 0.439560, Val Loss: 0.447877\n",
      "13:07:17 INFO   Epoch 3/5 - Batch 11000/64000 Done - Train Loss: 0.439206, Val Loss: 0.437317\n",
      "13:11:16 INFO   Epoch 3/5 - Batch 12000/64000 Done - Train Loss: 0.438902, Val Loss: 0.416940\n",
      "13:15:16 INFO   Epoch 3/5 - Batch 13000/64000 Done - Train Loss: 0.438714, Val Loss: 0.433923\n",
      "13:19:15 INFO   Epoch 3/5 - Batch 14000/64000 Done - Train Loss: 0.438902, Val Loss: 0.487230\n",
      "13:23:14 INFO   Epoch 3/5 - Batch 15000/64000 Done - Train Loss: 0.439224, Val Loss: 0.419919\n",
      "13:27:14 INFO   Epoch 3/5 - Batch 16000/64000 Done - Train Loss: 0.439359, Val Loss: 0.456892\n",
      "13:31:13 INFO   Epoch 3/5 - Batch 17000/64000 Done - Train Loss: 0.439572, Val Loss: 0.439240\n",
      "13:35:12 INFO   Epoch 3/5 - Batch 18000/64000 Done - Train Loss: 0.439689, Val Loss: 0.448368\n",
      "13:39:12 INFO   Epoch 3/5 - Batch 19000/64000 Done - Train Loss: 0.439905, Val Loss: 0.425266\n",
      "13:43:11 INFO   Epoch 3/5 - Batch 20000/64000 Done - Train Loss: 0.440032, Val Loss: 0.439802\n",
      "13:47:11 INFO   Epoch 3/5 - Batch 21000/64000 Done - Train Loss: 0.440145, Val Loss: 0.419085\n",
      "13:51:10 INFO   Epoch 3/5 - Batch 22000/64000 Done - Train Loss: 0.440160, Val Loss: 0.479131\n",
      "13:55:09 INFO   Epoch 3/5 - Batch 23000/64000 Done - Train Loss: 0.440245, Val Loss: 0.446759\n",
      "13:59:09 INFO   Epoch 3/5 - Batch 24000/64000 Done - Train Loss: 0.440338, Val Loss: 0.444133\n",
      "14:03:08 INFO   Epoch 3/5 - Batch 25000/64000 Done - Train Loss: 0.440434, Val Loss: 0.470925\n",
      "14:07:08 INFO   Epoch 3/5 - Batch 26000/64000 Done - Train Loss: 0.440546, Val Loss: 0.422888\n",
      "14:11:07 INFO   Epoch 3/5 - Batch 27000/64000 Done - Train Loss: 0.440656, Val Loss: 0.450749\n",
      "14:15:06 INFO   Epoch 3/5 - Batch 28000/64000 Done - Train Loss: 0.440744, Val Loss: 0.470678\n",
      "14:19:06 INFO   Epoch 3/5 - Batch 29000/64000 Done - Train Loss: 0.440865, Val Loss: 0.437973\n",
      "14:23:05 INFO   Epoch 3/5 - Batch 30000/64000 Done - Train Loss: 0.440921, Val Loss: 0.477762\n",
      "14:27:04 INFO   Epoch 3/5 - Batch 31000/64000 Done - Train Loss: 0.441050, Val Loss: 0.427548\n",
      "14:31:04 INFO   Epoch 3/5 - Batch 32000/64000 Done - Train Loss: 0.441141, Val Loss: 0.427766\n",
      "14:35:03 INFO   Epoch 3/5 - Batch 33000/64000 Done - Train Loss: 0.441131, Val Loss: 0.500231\n",
      "14:39:02 INFO   Epoch 3/5 - Batch 34000/64000 Done - Train Loss: 0.441120, Val Loss: 0.431409\n",
      "14:43:02 INFO   Epoch 3/5 - Batch 35000/64000 Done - Train Loss: 0.441126, Val Loss: 0.394890\n",
      "14:47:01 INFO   Epoch 3/5 - Batch 36000/64000 Done - Train Loss: 0.441080, Val Loss: 0.439374\n",
      "14:51:01 INFO   Epoch 3/5 - Batch 37000/64000 Done - Train Loss: 0.441132, Val Loss: 0.454588\n",
      "14:55:00 INFO   Epoch 3/5 - Batch 38000/64000 Done - Train Loss: 0.441107, Val Loss: 0.461022\n",
      "14:58:59 INFO   Epoch 3/5 - Batch 39000/64000 Done - Train Loss: 0.441104, Val Loss: 0.430326\n",
      "15:02:59 INFO   Epoch 3/5 - Batch 40000/64000 Done - Train Loss: 0.441066, Val Loss: 0.496489\n",
      "15:06:58 INFO   Epoch 3/5 - Batch 41000/64000 Done - Train Loss: 0.441042, Val Loss: 0.459906\n",
      "15:10:57 INFO   Epoch 3/5 - Batch 42000/64000 Done - Train Loss: 0.441010, Val Loss: 0.440191\n",
      "15:14:57 INFO   Epoch 3/5 - Batch 43000/64000 Done - Train Loss: 0.440961, Val Loss: 0.453557\n",
      "15:18:56 INFO   Epoch 3/5 - Batch 44000/64000 Done - Train Loss: 0.440943, Val Loss: 0.435840\n",
      "15:22:56 INFO   Epoch 3/5 - Batch 45000/64000 Done - Train Loss: 0.440933, Val Loss: 0.496946\n",
      "15:26:55 INFO   Epoch 3/5 - Batch 46000/64000 Done - Train Loss: 0.440888, Val Loss: 0.510377\n",
      "15:30:54 INFO   Epoch 3/5 - Batch 47000/64000 Done - Train Loss: 0.440814, Val Loss: 0.432396\n",
      "15:34:54 INFO   Epoch 3/5 - Batch 48000/64000 Done - Train Loss: 0.440780, Val Loss: 0.421692\n",
      "15:38:54 INFO   Epoch 3/5 - Batch 49000/64000 Done - Train Loss: 0.440739, Val Loss: 0.517209\n",
      "15:42:53 INFO   Epoch 3/5 - Batch 50000/64000 Done - Train Loss: 0.440671, Val Loss: 0.485615\n",
      "15:46:53 INFO   Epoch 3/5 - Batch 51000/64000 Done - Train Loss: 0.440624, Val Loss: 0.480755\n",
      "15:50:52 INFO   Epoch 3/5 - Batch 52000/64000 Done - Train Loss: 0.440678, Val Loss: 0.429400\n",
      "15:54:52 INFO   Epoch 3/5 - Batch 53000/64000 Done - Train Loss: 0.440728, Val Loss: 0.440575\n",
      "15:58:51 INFO   Epoch 3/5 - Batch 54000/64000 Done - Train Loss: 0.440762, Val Loss: 0.449198\n",
      "16:02:51 INFO   Epoch 3/5 - Batch 55000/64000 Done - Train Loss: 0.440848, Val Loss: 0.454979\n",
      "16:06:50 INFO   Epoch 3/5 - Batch 56000/64000 Done - Train Loss: 0.440895, Val Loss: 0.441661\n",
      "16:10:50 INFO   Epoch 3/5 - Batch 57000/64000 Done - Train Loss: 0.440942, Val Loss: 0.447111\n",
      "16:14:50 INFO   Epoch 3/5 - Batch 58000/64000 Done - Train Loss: 0.440979, Val Loss: 0.449591\n",
      "16:18:49 INFO   Epoch 3/5 - Batch 59000/64000 Done - Train Loss: 0.441003, Val Loss: 0.453699\n",
      "16:22:48 INFO   Epoch 3/5 - Batch 60000/64000 Done - Train Loss: 0.440996, Val Loss: 0.460047\n",
      "16:26:48 INFO   Epoch 3/5 - Batch 61000/64000 Done - Train Loss: 0.440998, Val Loss: 0.419568\n",
      "16:30:47 INFO   Epoch 3/5 - Batch 62000/64000 Done - Train Loss: 0.441015, Val Loss: 0.464603\n",
      "16:34:46 INFO   Epoch 3/5 - Batch 63000/64000 Done - Train Loss: 0.441010, Val Loss: 0.426958\n"
     ]
    }
   ],
   "source": [
    "execution.train_model_separate_inp(DCN, \n",
    "                                   train_data, \n",
    "                                   train_data, \n",
    "                                   F.binary_cross_entropy_with_logits, \n",
    "                                   torch.optim.Adam(DCN.parameters()), \n",
    "                                   DEVICE, \n",
    "                                   checkpoint_dir, \n",
    "                                   checkpoint_prefix \n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DCN_2', 'DCN_4', 'DCN_5', 'DCN_3', 'DCN_1']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCN_BinClf_Torch.DCN_Layer(len(embedding_map_dict)+60,\n",
    "                                 20,\n",
    "                                 26,\n",
    "                                 13,\n",
    "                                 [1024 for _ in range(2)],\n",
    "                                 [0.5 for _ in range(3)],\n",
    "                                 6).to(DEVICE)\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'DCN_1')))\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Model EP1 ROC Score: {:.6f}'.format(execution.get_roc_auc_score_separate_inp(model, test_data, DEVICE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCN_BinClf_Torch.DCN_Layer(len(embedding_map_dict)+60,\n",
    "                                 20,\n",
    "                                 26,\n",
    "                                 13,\n",
    "                                 [1024 for _ in range(2)],\n",
    "                                 [0.5 for _ in range(3)],\n",
    "                                 6).to(DEVICE)\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'DCN_2')))\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Model ROC Score: {:.6f}'.format(execution.get_roc_auc_score_separate_inp(model, test_data, DEVICE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCN_BinClf_Torch.DCN_Layer(len(embedding_map_dict)+60,\n",
    "                                 20,\n",
    "                                 26,\n",
    "                                 13,\n",
    "                                 [1024 for _ in range(2)],\n",
    "                                 [0.5 for _ in range(3)],\n",
    "                                 6).to(DEVICE)\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'DCN_3')))\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Model ROC Score: {:.6f}'.format(execution.get_roc_auc_score_separate_inp(model, test_data, DEVICE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCN_BinClf_Torch.DCN_Layer(len(embedding_map_dict)+60,\n",
    "                                 20,\n",
    "                                 26,\n",
    "                                 13,\n",
    "                                 [1024 for _ in range(2)],\n",
    "                                 [0.5 for _ in range(3)],\n",
    "                                 6).to(DEVICE)\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'DCN_4')))\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Model ROC Score: {:.6f}'.format(execution.get_roc_auc_score_separate_inp(model, test_data, DEVICE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCN_BinClf_Torch.DCN_Layer(len(embedding_map_dict)+60,\n",
    "                                 20,\n",
    "                                 26,\n",
    "                                 13,\n",
    "                                 [1024 for _ in range(2)],\n",
    "                                 [0.5 for _ in range(3)],\n",
    "                                 6).to(DEVICE)\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'DCN_5')))\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Model ROC Score: {:.6f}'.format(execution.get_roc_auc_score_separate_inp(model, test_data, DEVICE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Performance on Training Data as Well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCN_BinClf_Torch.DCN_Layer(len(embedding_map_dict)+60,\n",
    "                                 20,\n",
    "                                 26,\n",
    "                                 13,\n",
    "                                 [1024 for _ in range(2)],\n",
    "                                 [0.5 for _ in range(3)],\n",
    "                                 6).to(DEVICE)\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'DCN_1')))\n",
    "model = model.to(DEVICE)\n",
    "logger.info('Model ROC Score: {:.6f}'.format(execution.get_roc_auc_score_separate_inp(model, train_data, DEVICE)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
