{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "assert torch.__version__>='1.2.0', 'Expect PyTorch>=1.2.0 but get {}'.format(torch.__version__)\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "imp_dir = '../Implementations'\n",
    "sys.path.insert(1, imp_dir)\n",
    "data_dir = '../Data/criteo'\n",
    "sys.path.insert(1, data_dir)\n",
    "\n",
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging)\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, format='%(asctime)s %(levelname)-6s %(message)s', level=logging.INFO, datefmt='%H:%M:%S')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:10:18 INFO   Device in Use: cuda\n",
      "00:10:18 INFO   CUDA Memory: Total 11.17 GB, Cached 0.00 GB, Allocated 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info('Device in Use: {}'.format(DEVICE))\n",
    "torch.cuda.empty_cache()\n",
    "t = torch.cuda.get_device_properties(DEVICE).total_memory/1024**3\n",
    "c = torch.cuda.memory_cached(DEVICE)/1024**3\n",
    "a = torch.cuda.memory_allocated(DEVICE)/1024**3\n",
    "logger.info('CUDA Memory: Total {:.2f} GB, Cached {:.2f} GB, Allocated {:.2f} GB'.format(t,c,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map_dict_pkl_path = os.path.join(data_dir, 'criteo_feature_dict_artifact/categorical_feature_map_dict.pkl')\n",
    "with open(embedding_map_dict_pkl_path, 'rb') as f:\n",
    "    embedding_map_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all available files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_artifact_dir = os.path.join(data_dir, 'criteo_train_numpy_artifact')\n",
    "index_artifact = sorted(list(filter(lambda x: x.split('-')[1]=='index', os.listdir(np_artifact_dir))), key = lambda x: int(x.split('.')[0].split('-')[-1]))\n",
    "value_artifact = sorted(list(filter(lambda x: x.split('-')[1]=='value', os.listdir(np_artifact_dir))), key = lambda x: int(x.split('.')[0].split('-')[-1]))\n",
    "label_artifact = sorted(list(filter(lambda x: x.split('-')[1]=='label', os.listdir(np_artifact_dir))), key = lambda x: int(x.split('.')[0].split('-')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_data = (\n",
    "    np.vstack([np.load(os.path.join(np_artifact_dir, f)) for f in index_artifact[:10]]),\n",
    "    np.vstack([np.load(os.path.join(np_artifact_dir, f)) for f in value_artifact[:10]]),\n",
    "    np.vstack([np.load(os.path.join(np_artifact_dir, f)) for f in label_artifact[:10]]),\n",
    ")\n",
    "\n",
    "logger.info('Training data loaded after {:.2f}s'.format(time.time()-start))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "test_data = (\n",
    "    np.vstack([np.load(os.path.join(np_artifact_dir, f)) for f in index_artifact[10:]]),\n",
    "    np.vstack([np.load(os.path.join(np_artifact_dir, f)) for f in value_artifact[10:]]),\n",
    "    np.vstack([np.load(os.path.join(np_artifact_dir, f)) for f in label_artifact[10:]]),\n",
    ")\n",
    "\n",
    "logger.info('Test data loaded after {:.2f}s'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'DCN_BinClf_Torch' from '../Implementations/DCN_BinClf_Torch.py'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import execution\n",
    "importlib.reload(execution)\n",
    "import DCN_BinClf_Torch \n",
    "importlib.reload(DCN_BinClf_Torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCN = DCN_BinClf_Torch.DCN_Layer(len(embedding_map_dict)+60,\n",
    "                                 20,\n",
    "                                 26,\n",
    "                                 13,\n",
    "                                 [1024 for _ in range(2)],\n",
    "                                 [0.5 for _ in range(3)],\n",
    "                                 6).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "checkpoint_dir = os.path.join(cwd, 'DCN_artifact')\n",
    "checkpoint_prefix = 'DCN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:06:59 INFO   Epoch 1/5 - Batch 1/6400 Done - Train Loss: 0.708079, Val Loss: 1.322342\n",
      "00:07:00 INFO   Epoch 1/5 - Batch 2/6400 Done - Train Loss: 0.673500, Val Loss: 0.687095\n",
      "00:07:00 INFO   Epoch 1/5 - Batch 3/6400 Done - Train Loss: 0.639062, Val Loss: 0.655121\n",
      "00:07:00 INFO   Epoch 1/5 - Batch 4/6400 Done - Train Loss: 0.646194, Val Loss: 0.633870\n",
      "00:07:01 INFO   Epoch 1/5 - Batch 5/6400 Done - Train Loss: 0.639051, Val Loss: 0.625099\n",
      "00:07:01 INFO   Epoch 1/5 - Batch 6/6400 Done - Train Loss: 0.636626, Val Loss: 0.613462\n",
      "00:07:01 INFO   Epoch 1/5 - Batch 7/6400 Done - Train Loss: 0.635699, Val Loss: 0.603782\n",
      "00:07:01 INFO   Epoch 1/5 - Batch 8/6400 Done - Train Loss: 0.632878, Val Loss: 0.594425\n",
      "00:07:02 INFO   Epoch 1/5 - Batch 9/6400 Done - Train Loss: 0.632071, Val Loss: 0.592475\n",
      "00:07:02 INFO   Epoch 1/5 - Batch 10/6400 Done - Train Loss: 0.628984, Val Loss: 0.605773\n",
      "00:07:02 INFO   Epoch 1/5 - Batch 11/6400 Done - Train Loss: 0.619818, Val Loss: 0.584357\n",
      "00:07:03 INFO   Epoch 1/5 - Batch 12/6400 Done - Train Loss: 0.611390, Val Loss: 0.580145\n",
      "00:07:03 INFO   Epoch 1/5 - Batch 13/6400 Done - Train Loss: 0.609551, Val Loss: 0.572406\n",
      "00:07:03 INFO   Epoch 1/5 - Batch 14/6400 Done - Train Loss: 0.605660, Val Loss: 0.572899\n",
      "00:07:04 INFO   Epoch 1/5 - Batch 15/6400 Done - Train Loss: 0.604153, Val Loss: 0.577127\n",
      "00:07:04 INFO   Epoch 1/5 - Batch 16/6400 Done - Train Loss: 0.598785, Val Loss: 0.576382\n",
      "00:07:04 INFO   Epoch 1/5 - Batch 17/6400 Done - Train Loss: 0.594193, Val Loss: 0.572666\n",
      "00:07:05 INFO   Epoch 1/5 - Batch 18/6400 Done - Train Loss: 0.593382, Val Loss: 0.568835\n",
      "00:07:05 INFO   Epoch 1/5 - Batch 19/6400 Done - Train Loss: 0.593081, Val Loss: 0.546383\n",
      "00:07:05 INFO   Epoch 1/5 - Batch 20/6400 Done - Train Loss: 0.591177, Val Loss: 0.552722\n",
      "00:07:06 INFO   Epoch 1/5 - Batch 21/6400 Done - Train Loss: 0.591366, Val Loss: 0.535621\n",
      "00:07:06 INFO   Epoch 1/5 - Batch 22/6400 Done - Train Loss: 0.591780, Val Loss: 0.568002\n",
      "00:07:06 INFO   Epoch 1/5 - Batch 23/6400 Done - Train Loss: 0.593218, Val Loss: 0.536618\n",
      "00:07:07 INFO   Epoch 1/5 - Batch 24/6400 Done - Train Loss: 0.591529, Val Loss: 0.536042\n",
      "00:07:07 INFO   Epoch 1/5 - Batch 25/6400 Done - Train Loss: 0.594182, Val Loss: 0.552619\n",
      "00:07:07 INFO   Epoch 1/5 - Batch 26/6400 Done - Train Loss: 0.591843, Val Loss: 0.570173\n",
      "00:07:07 INFO   Epoch 1/5 - Batch 27/6400 Done - Train Loss: 0.588685, Val Loss: 0.538738\n",
      "00:07:08 INFO   Epoch 1/5 - Batch 28/6400 Done - Train Loss: 0.587492, Val Loss: 0.536901\n",
      "00:07:08 INFO   Epoch 1/5 - Batch 29/6400 Done - Train Loss: 0.587462, Val Loss: 0.527944\n",
      "00:07:08 INFO   Epoch 1/5 - Batch 30/6400 Done - Train Loss: 0.586558, Val Loss: 0.555663\n",
      "00:07:09 INFO   Epoch 1/5 - Batch 31/6400 Done - Train Loss: 0.586400, Val Loss: 0.551134\n",
      "00:07:09 INFO   Epoch 1/5 - Batch 32/6400 Done - Train Loss: 0.585465, Val Loss: 0.557077\n",
      "00:07:09 INFO   Epoch 1/5 - Batch 33/6400 Done - Train Loss: 0.584118, Val Loss: 0.563895\n",
      "00:07:10 INFO   Epoch 1/5 - Batch 34/6400 Done - Train Loss: 0.583303, Val Loss: 0.553879\n",
      "00:07:10 INFO   Epoch 1/5 - Batch 35/6400 Done - Train Loss: 0.583016, Val Loss: 0.535020\n",
      "00:07:10 INFO   Epoch 1/5 - Batch 36/6400 Done - Train Loss: 0.583035, Val Loss: 0.559683\n",
      "00:07:11 INFO   Epoch 1/5 - Batch 37/6400 Done - Train Loss: 0.582509, Val Loss: 0.566972\n",
      "00:07:11 INFO   Epoch 1/5 - Batch 38/6400 Done - Train Loss: 0.581238, Val Loss: 0.579573\n",
      "00:07:11 INFO   Epoch 1/5 - Batch 39/6400 Done - Train Loss: 0.579536, Val Loss: 0.557767\n",
      "00:07:12 INFO   Epoch 1/5 - Batch 40/6400 Done - Train Loss: 0.579544, Val Loss: 0.559896\n",
      "00:07:12 INFO   Epoch 1/5 - Batch 41/6400 Done - Train Loss: 0.578131, Val Loss: 0.567675\n",
      "00:07:12 INFO   Epoch 1/5 - Batch 42/6400 Done - Train Loss: 0.576662, Val Loss: 0.580830\n",
      "00:07:13 INFO   Epoch 1/5 - Batch 43/6400 Done - Train Loss: 0.575913, Val Loss: 0.554950\n",
      "00:07:13 INFO   Epoch 1/5 - Batch 44/6400 Done - Train Loss: 0.575303, Val Loss: 0.573288\n",
      "00:07:13 INFO   Epoch 1/5 - Batch 45/6400 Done - Train Loss: 0.573334, Val Loss: 0.549437\n",
      "00:07:13 INFO   Epoch 1/5 - Batch 46/6400 Done - Train Loss: 0.572685, Val Loss: 0.543598\n",
      "00:07:14 INFO   Epoch 1/5 - Batch 47/6400 Done - Train Loss: 0.572198, Val Loss: 0.552316\n",
      "00:07:14 INFO   Epoch 1/5 - Batch 48/6400 Done - Train Loss: 0.571336, Val Loss: 0.551441\n",
      "00:07:14 INFO   Epoch 1/5 - Batch 49/6400 Done - Train Loss: 0.571586, Val Loss: 0.558711\n",
      "00:07:15 INFO   Epoch 1/5 - Batch 50/6400 Done - Train Loss: 0.570083, Val Loss: 0.550546\n",
      "00:07:15 INFO   Epoch 1/5 - Batch 51/6400 Done - Train Loss: 0.569426, Val Loss: 0.529601\n",
      "00:07:15 INFO   Epoch 1/5 - Batch 52/6400 Done - Train Loss: 0.569115, Val Loss: 0.566341\n",
      "00:07:16 INFO   Epoch 1/5 - Batch 53/6400 Done - Train Loss: 0.568902, Val Loss: 0.535668\n",
      "00:07:16 INFO   Epoch 1/5 - Batch 54/6400 Done - Train Loss: 0.568363, Val Loss: 0.543536\n",
      "00:07:16 INFO   Epoch 1/5 - Batch 55/6400 Done - Train Loss: 0.567478, Val Loss: 0.538983\n",
      "00:07:17 INFO   Epoch 1/5 - Batch 56/6400 Done - Train Loss: 0.566169, Val Loss: 0.541429\n",
      "00:07:17 INFO   Epoch 1/5 - Batch 57/6400 Done - Train Loss: 0.565868, Val Loss: 0.539966\n",
      "00:07:17 INFO   Epoch 1/5 - Batch 58/6400 Done - Train Loss: 0.565065, Val Loss: 0.525162\n",
      "00:07:18 INFO   Epoch 1/5 - Batch 59/6400 Done - Train Loss: 0.564346, Val Loss: 0.552033\n",
      "00:07:18 INFO   Epoch 1/5 - Batch 60/6400 Done - Train Loss: 0.563372, Val Loss: 0.540483\n",
      "00:07:18 INFO   Epoch 1/5 - Batch 61/6400 Done - Train Loss: 0.563218, Val Loss: 0.552729\n",
      "00:07:18 INFO   Epoch 1/5 - Batch 62/6400 Done - Train Loss: 0.563072, Val Loss: 0.533896\n",
      "00:07:19 INFO   Epoch 1/5 - Batch 63/6400 Done - Train Loss: 0.561883, Val Loss: 0.535483\n",
      "00:07:19 INFO   Epoch 1/5 - Batch 64/6400 Done - Train Loss: 0.561417, Val Loss: 0.547078\n",
      "00:07:19 INFO   Epoch 1/5 - Batch 65/6400 Done - Train Loss: 0.560552, Val Loss: 0.545998\n",
      "00:07:20 INFO   Epoch 1/5 - Batch 66/6400 Done - Train Loss: 0.559465, Val Loss: 0.540624\n",
      "00:07:20 INFO   Epoch 1/5 - Batch 67/6400 Done - Train Loss: 0.558943, Val Loss: 0.535882\n",
      "00:07:20 INFO   Epoch 1/5 - Batch 68/6400 Done - Train Loss: 0.558678, Val Loss: 0.530404\n",
      "00:07:21 INFO   Epoch 1/5 - Batch 69/6400 Done - Train Loss: 0.557648, Val Loss: 0.548048\n",
      "00:07:21 INFO   Epoch 1/5 - Batch 70/6400 Done - Train Loss: 0.557314, Val Loss: 0.541515\n",
      "00:07:21 INFO   Epoch 1/5 - Batch 71/6400 Done - Train Loss: 0.556837, Val Loss: 0.548394\n",
      "00:07:22 INFO   Epoch 1/5 - Batch 72/6400 Done - Train Loss: 0.556507, Val Loss: 0.539576\n",
      "00:07:22 INFO   Epoch 1/5 - Batch 73/6400 Done - Train Loss: 0.555797, Val Loss: 0.537757\n",
      "00:07:22 INFO   Epoch 1/5 - Batch 74/6400 Done - Train Loss: 0.555619, Val Loss: 0.526710\n",
      "00:07:23 INFO   Epoch 1/5 - Batch 75/6400 Done - Train Loss: 0.555106, Val Loss: 0.509830\n",
      "00:07:23 INFO   Epoch 1/5 - Batch 76/6400 Done - Train Loss: 0.554463, Val Loss: 0.529488\n",
      "00:07:23 INFO   Epoch 1/5 - Batch 77/6400 Done - Train Loss: 0.554248, Val Loss: 0.508599\n",
      "00:07:24 INFO   Epoch 1/5 - Batch 78/6400 Done - Train Loss: 0.554291, Val Loss: 0.507600\n",
      "00:07:24 INFO   Epoch 1/5 - Batch 79/6400 Done - Train Loss: 0.554069, Val Loss: 0.539746\n",
      "00:07:24 INFO   Epoch 1/5 - Batch 80/6400 Done - Train Loss: 0.553850, Val Loss: 0.503895\n",
      "00:07:24 INFO   Epoch 1/5 - Batch 81/6400 Done - Train Loss: 0.553360, Val Loss: 0.519065\n",
      "00:07:25 INFO   Epoch 1/5 - Batch 82/6400 Done - Train Loss: 0.552997, Val Loss: 0.527144\n",
      "00:07:25 INFO   Epoch 1/5 - Batch 83/6400 Done - Train Loss: 0.552520, Val Loss: 0.534769\n",
      "00:07:25 INFO   Epoch 1/5 - Batch 84/6400 Done - Train Loss: 0.552203, Val Loss: 0.517112\n",
      "00:07:26 INFO   Epoch 1/5 - Batch 85/6400 Done - Train Loss: 0.551645, Val Loss: 0.519704\n",
      "00:07:26 INFO   Epoch 1/5 - Batch 86/6400 Done - Train Loss: 0.551235, Val Loss: 0.535660\n",
      "00:07:26 INFO   Epoch 1/5 - Batch 87/6400 Done - Train Loss: 0.550840, Val Loss: 0.529523\n",
      "00:07:27 INFO   Epoch 1/5 - Batch 88/6400 Done - Train Loss: 0.549989, Val Loss: 0.544649\n",
      "00:07:27 INFO   Epoch 1/5 - Batch 89/6400 Done - Train Loss: 0.549711, Val Loss: 0.518957\n",
      "00:07:27 INFO   Epoch 1/5 - Batch 90/6400 Done - Train Loss: 0.549615, Val Loss: 0.525630\n",
      "00:07:28 INFO   Epoch 1/5 - Batch 91/6400 Done - Train Loss: 0.549168, Val Loss: 0.495608\n",
      "00:07:28 INFO   Epoch 1/5 - Batch 92/6400 Done - Train Loss: 0.549208, Val Loss: 0.520730\n",
      "00:07:28 INFO   Epoch 1/5 - Batch 93/6400 Done - Train Loss: 0.548917, Val Loss: 0.500238\n",
      "00:07:29 INFO   Epoch 1/5 - Batch 94/6400 Done - Train Loss: 0.548288, Val Loss: 0.562081\n",
      "00:07:29 INFO   Epoch 1/5 - Batch 95/6400 Done - Train Loss: 0.547883, Val Loss: 0.516133\n",
      "00:07:29 INFO   Epoch 1/5 - Batch 96/6400 Done - Train Loss: 0.547727, Val Loss: 0.498248\n",
      "00:07:30 INFO   Epoch 1/5 - Batch 97/6400 Done - Train Loss: 0.547766, Val Loss: 0.503622\n",
      "00:07:30 INFO   Epoch 1/5 - Batch 98/6400 Done - Train Loss: 0.547293, Val Loss: 0.524521\n",
      "00:07:30 INFO   Epoch 1/5 - Batch 99/6400 Done - Train Loss: 0.547107, Val Loss: 0.509846\n",
      "00:07:30 INFO   Epoch 1/5 - Batch 100/6400 Done - Train Loss: 0.547024, Val Loss: 0.539108\n",
      "00:07:31 INFO   Epoch 1/5 - Batch 101/6400 Done - Train Loss: 0.546901, Val Loss: 0.515571\n",
      "00:07:31 INFO   Epoch 1/5 - Batch 102/6400 Done - Train Loss: 0.546128, Val Loss: 0.508686\n",
      "00:07:31 INFO   Epoch 1/5 - Batch 103/6400 Done - Train Loss: 0.545756, Val Loss: 0.530125\n",
      "00:07:32 INFO   Epoch 1/5 - Batch 104/6400 Done - Train Loss: 0.545579, Val Loss: 0.531992\n",
      "00:07:32 INFO   Epoch 1/5 - Batch 105/6400 Done - Train Loss: 0.544988, Val Loss: 0.523578\n",
      "00:07:32 INFO   Epoch 1/5 - Batch 106/6400 Done - Train Loss: 0.544750, Val Loss: 0.503711\n",
      "00:07:33 INFO   Epoch 1/5 - Batch 107/6400 Done - Train Loss: 0.544211, Val Loss: 0.509087\n",
      "00:07:33 INFO   Epoch 1/5 - Batch 108/6400 Done - Train Loss: 0.543936, Val Loss: 0.545254\n",
      "00:07:33 INFO   Epoch 1/5 - Batch 109/6400 Done - Train Loss: 0.543532, Val Loss: 0.502175\n",
      "00:07:34 INFO   Epoch 1/5 - Batch 110/6400 Done - Train Loss: 0.543094, Val Loss: 0.523335\n",
      "00:07:34 INFO   Epoch 1/5 - Batch 111/6400 Done - Train Loss: 0.542656, Val Loss: 0.477732\n",
      "00:07:34 INFO   Epoch 1/5 - Batch 112/6400 Done - Train Loss: 0.542467, Val Loss: 0.519943\n",
      "00:07:35 INFO   Epoch 1/5 - Batch 113/6400 Done - Train Loss: 0.541972, Val Loss: 0.507825\n",
      "00:07:35 INFO   Epoch 1/5 - Batch 114/6400 Done - Train Loss: 0.541979, Val Loss: 0.526067\n",
      "00:07:35 INFO   Epoch 1/5 - Batch 115/6400 Done - Train Loss: 0.541856, Val Loss: 0.504252\n",
      "00:07:36 INFO   Epoch 1/5 - Batch 116/6400 Done - Train Loss: 0.542022, Val Loss: 0.523646\n",
      "00:07:36 INFO   Epoch 1/5 - Batch 117/6400 Done - Train Loss: 0.541668, Val Loss: 0.534751\n",
      "00:07:36 INFO   Epoch 1/5 - Batch 118/6400 Done - Train Loss: 0.541517, Val Loss: 0.526976\n",
      "00:07:36 INFO   Epoch 1/5 - Batch 119/6400 Done - Train Loss: 0.541501, Val Loss: 0.519171\n",
      "00:07:37 INFO   Epoch 1/5 - Batch 120/6400 Done - Train Loss: 0.541164, Val Loss: 0.538171\n",
      "00:07:37 INFO   Epoch 1/5 - Batch 121/6400 Done - Train Loss: 0.540901, Val Loss: 0.520084\n",
      "00:07:37 INFO   Epoch 1/5 - Batch 122/6400 Done - Train Loss: 0.540713, Val Loss: 0.493508\n",
      "00:07:38 INFO   Epoch 1/5 - Batch 123/6400 Done - Train Loss: 0.540718, Val Loss: 0.528894\n",
      "00:07:38 INFO   Epoch 1/5 - Batch 124/6400 Done - Train Loss: 0.540420, Val Loss: 0.522481\n",
      "00:07:38 INFO   Epoch 1/5 - Batch 125/6400 Done - Train Loss: 0.540292, Val Loss: 0.523153\n",
      "00:07:39 INFO   Epoch 1/5 - Batch 126/6400 Done - Train Loss: 0.540197, Val Loss: 0.545693\n",
      "00:07:39 INFO   Epoch 1/5 - Batch 127/6400 Done - Train Loss: 0.539820, Val Loss: 0.494207\n",
      "00:07:39 INFO   Epoch 1/5 - Batch 128/6400 Done - Train Loss: 0.539832, Val Loss: 0.458962\n",
      "00:07:40 INFO   Epoch 1/5 - Batch 129/6400 Done - Train Loss: 0.539524, Val Loss: 0.507378\n",
      "00:07:40 INFO   Epoch 1/5 - Batch 130/6400 Done - Train Loss: 0.538913, Val Loss: 0.488515\n",
      "00:07:40 INFO   Epoch 1/5 - Batch 131/6400 Done - Train Loss: 0.538800, Val Loss: 0.464391\n",
      "00:07:41 INFO   Epoch 1/5 - Batch 132/6400 Done - Train Loss: 0.538557, Val Loss: 0.514932\n",
      "00:07:41 INFO   Epoch 1/5 - Batch 133/6400 Done - Train Loss: 0.538465, Val Loss: 0.504972\n",
      "00:07:41 INFO   Epoch 1/5 - Batch 134/6400 Done - Train Loss: 0.538398, Val Loss: 0.502486\n",
      "00:07:42 INFO   Epoch 1/5 - Batch 135/6400 Done - Train Loss: 0.537790, Val Loss: 0.523673\n",
      "00:07:42 INFO   Epoch 1/5 - Batch 136/6400 Done - Train Loss: 0.537620, Val Loss: 0.519633\n",
      "00:07:42 INFO   Epoch 1/5 - Batch 137/6400 Done - Train Loss: 0.537411, Val Loss: 0.502031\n",
      "00:07:42 INFO   Epoch 1/5 - Batch 138/6400 Done - Train Loss: 0.537126, Val Loss: 0.479887\n",
      "00:07:43 INFO   Epoch 1/5 - Batch 139/6400 Done - Train Loss: 0.537027, Val Loss: 0.496858\n",
      "00:07:43 INFO   Epoch 1/5 - Batch 140/6400 Done - Train Loss: 0.536934, Val Loss: 0.493141\n",
      "00:07:43 INFO   Epoch 1/5 - Batch 141/6400 Done - Train Loss: 0.536999, Val Loss: 0.489529\n",
      "00:07:44 INFO   Epoch 1/5 - Batch 142/6400 Done - Train Loss: 0.536804, Val Loss: 0.506618\n",
      "00:07:44 INFO   Epoch 1/5 - Batch 143/6400 Done - Train Loss: 0.536748, Val Loss: 0.510912\n",
      "00:07:44 INFO   Epoch 1/5 - Batch 144/6400 Done - Train Loss: 0.536836, Val Loss: 0.499504\n",
      "00:07:45 INFO   Epoch 1/5 - Batch 145/6400 Done - Train Loss: 0.536479, Val Loss: 0.505138\n",
      "00:07:45 INFO   Epoch 1/5 - Batch 146/6400 Done - Train Loss: 0.536298, Val Loss: 0.470856\n",
      "00:07:45 INFO   Epoch 1/5 - Batch 147/6400 Done - Train Loss: 0.535976, Val Loss: 0.504243\n",
      "00:07:46 INFO   Epoch 1/5 - Batch 148/6400 Done - Train Loss: 0.536007, Val Loss: 0.501362\n",
      "00:07:46 INFO   Epoch 1/5 - Batch 149/6400 Done - Train Loss: 0.535630, Val Loss: 0.491930\n",
      "00:07:46 INFO   Epoch 1/5 - Batch 150/6400 Done - Train Loss: 0.535653, Val Loss: 0.514289\n",
      "00:07:47 INFO   Epoch 1/5 - Batch 151/6400 Done - Train Loss: 0.535215, Val Loss: 0.485295\n",
      "00:07:47 INFO   Epoch 1/5 - Batch 152/6400 Done - Train Loss: 0.535142, Val Loss: 0.504297\n",
      "00:07:47 INFO   Epoch 1/5 - Batch 153/6400 Done - Train Loss: 0.534899, Val Loss: 0.514271\n",
      "00:07:48 INFO   Epoch 1/5 - Batch 154/6400 Done - Train Loss: 0.534857, Val Loss: 0.456450\n",
      "00:07:48 INFO   Epoch 1/5 - Batch 155/6400 Done - Train Loss: 0.534598, Val Loss: 0.486003\n",
      "00:07:48 INFO   Epoch 1/5 - Batch 156/6400 Done - Train Loss: 0.534274, Val Loss: 0.518725\n",
      "00:07:48 INFO   Epoch 1/5 - Batch 157/6400 Done - Train Loss: 0.533875, Val Loss: 0.522996\n",
      "00:07:49 INFO   Epoch 1/5 - Batch 158/6400 Done - Train Loss: 0.533697, Val Loss: 0.514333\n",
      "00:07:49 INFO   Epoch 1/5 - Batch 159/6400 Done - Train Loss: 0.533690, Val Loss: 0.490285\n",
      "00:07:49 INFO   Epoch 1/5 - Batch 160/6400 Done - Train Loss: 0.533392, Val Loss: 0.519084\n",
      "00:07:50 INFO   Epoch 1/5 - Batch 161/6400 Done - Train Loss: 0.533336, Val Loss: 0.555294\n",
      "00:07:50 INFO   Epoch 1/5 - Batch 162/6400 Done - Train Loss: 0.532990, Val Loss: 0.537358\n",
      "00:07:50 INFO   Epoch 1/5 - Batch 163/6400 Done - Train Loss: 0.533034, Val Loss: 0.480922\n",
      "00:07:51 INFO   Epoch 1/5 - Batch 164/6400 Done - Train Loss: 0.532780, Val Loss: 0.515440\n",
      "00:07:51 INFO   Epoch 1/5 - Batch 165/6400 Done - Train Loss: 0.532731, Val Loss: 0.484791\n",
      "00:07:51 INFO   Epoch 1/5 - Batch 166/6400 Done - Train Loss: 0.532394, Val Loss: 0.508627\n",
      "00:07:52 INFO   Epoch 1/5 - Batch 167/6400 Done - Train Loss: 0.532388, Val Loss: 0.507396\n",
      "00:07:52 INFO   Epoch 1/5 - Batch 168/6400 Done - Train Loss: 0.531999, Val Loss: 0.489859\n",
      "00:07:52 INFO   Epoch 1/5 - Batch 169/6400 Done - Train Loss: 0.531773, Val Loss: 0.529936\n",
      "00:07:53 INFO   Epoch 1/5 - Batch 170/6400 Done - Train Loss: 0.531903, Val Loss: 0.482326\n",
      "00:07:53 INFO   Epoch 1/5 - Batch 171/6400 Done - Train Loss: 0.531766, Val Loss: 0.490822\n",
      "00:07:53 INFO   Epoch 1/5 - Batch 172/6400 Done - Train Loss: 0.531479, Val Loss: 0.516722\n",
      "00:07:54 INFO   Epoch 1/5 - Batch 173/6400 Done - Train Loss: 0.531252, Val Loss: 0.524453\n",
      "00:07:54 INFO   Epoch 1/5 - Batch 174/6400 Done - Train Loss: 0.531041, Val Loss: 0.499298\n",
      "00:07:54 INFO   Epoch 1/5 - Batch 175/6400 Done - Train Loss: 0.530970, Val Loss: 0.536547\n",
      "00:07:54 INFO   Epoch 1/5 - Batch 176/6400 Done - Train Loss: 0.530851, Val Loss: 0.503729\n",
      "00:07:55 INFO   Epoch 1/5 - Batch 177/6400 Done - Train Loss: 0.530650, Val Loss: 0.492512\n",
      "00:07:55 INFO   Epoch 1/5 - Batch 178/6400 Done - Train Loss: 0.530309, Val Loss: 0.512192\n",
      "00:07:55 INFO   Epoch 1/5 - Batch 179/6400 Done - Train Loss: 0.529998, Val Loss: 0.497381\n",
      "00:07:56 INFO   Epoch 1/5 - Batch 180/6400 Done - Train Loss: 0.529823, Val Loss: 0.499884\n",
      "00:07:56 INFO   Epoch 1/5 - Batch 181/6400 Done - Train Loss: 0.529801, Val Loss: 0.529502\n",
      "00:07:56 INFO   Epoch 1/5 - Batch 182/6400 Done - Train Loss: 0.529553, Val Loss: 0.481595\n",
      "00:07:57 INFO   Epoch 1/5 - Batch 183/6400 Done - Train Loss: 0.529418, Val Loss: 0.494593\n",
      "00:07:57 INFO   Epoch 1/5 - Batch 184/6400 Done - Train Loss: 0.529360, Val Loss: 0.486675\n",
      "00:07:57 INFO   Epoch 1/5 - Batch 185/6400 Done - Train Loss: 0.529295, Val Loss: 0.448111\n",
      "00:07:58 INFO   Epoch 1/5 - Batch 186/6400 Done - Train Loss: 0.529103, Val Loss: 0.534770\n",
      "00:07:58 INFO   Epoch 1/5 - Batch 187/6400 Done - Train Loss: 0.528852, Val Loss: 0.500455\n",
      "00:07:58 INFO   Epoch 1/5 - Batch 188/6400 Done - Train Loss: 0.528749, Val Loss: 0.491200\n",
      "00:07:59 INFO   Epoch 1/5 - Batch 189/6400 Done - Train Loss: 0.528556, Val Loss: 0.527146\n",
      "00:07:59 INFO   Epoch 1/5 - Batch 190/6400 Done - Train Loss: 0.528485, Val Loss: 0.490283\n",
      "00:07:59 INFO   Epoch 1/5 - Batch 191/6400 Done - Train Loss: 0.528261, Val Loss: 0.493152\n",
      "00:08:00 INFO   Epoch 1/5 - Batch 192/6400 Done - Train Loss: 0.528142, Val Loss: 0.494655\n",
      "00:08:00 INFO   Epoch 1/5 - Batch 193/6400 Done - Train Loss: 0.527862, Val Loss: 0.480862\n",
      "00:08:00 INFO   Epoch 1/5 - Batch 194/6400 Done - Train Loss: 0.527643, Val Loss: 0.516272\n",
      "00:08:00 INFO   Epoch 1/5 - Batch 195/6400 Done - Train Loss: 0.527725, Val Loss: 0.470949\n",
      "00:08:01 INFO   Epoch 1/5 - Batch 196/6400 Done - Train Loss: 0.527607, Val Loss: 0.508153\n",
      "00:08:01 INFO   Epoch 1/5 - Batch 197/6400 Done - Train Loss: 0.527228, Val Loss: 0.495835\n",
      "00:08:01 INFO   Epoch 1/5 - Batch 198/6400 Done - Train Loss: 0.527151, Val Loss: 0.506352\n",
      "00:08:02 INFO   Epoch 1/5 - Batch 199/6400 Done - Train Loss: 0.526891, Val Loss: 0.506733\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-8c02360c8b4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                    \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                    \u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                    \u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                                   )\n",
      "\u001b[0;32m~/SageMaker/RecSys-Notes/Data/criteo/execution.py\u001b[0m in \u001b[0;36mtrain_model_separate_inp\u001b[0;34m(model, train_data, test_data, loss_fn, optimizer, device, checkpoint_dir, checkpoint_prefix)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_sparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/RecSys-Notes/Implementations/DCN_BinClf_Torch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp_dense, inp_sparse)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_dense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_sparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0minp_sparse_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_sparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (None, sparse_dim) -> (None, sparse_dim * sparse_embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_sparse_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m                                \u001b[0;31m# (None, dense_dim + sparse_dim * sparse_embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m                                                         \u001b[0;31m# (None, dense_dim + sparse_dim * sparse_embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "execution.train_model_separate_inp(DCN, \n",
    "                                   train_data, \n",
    "                                   train_data, \n",
    "                                   F.binary_cross_entropy_with_logits, \n",
    "                                   torch.optim.Adam(DCN.parameters()), \n",
    "                                   DEVICE, \n",
    "                                   checkpoint_dir, \n",
    "                                   checkpoint_prefix\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DeepFM_1', 'DeepFM_2', 'DeepFM_5', 'DeepFM_4', 'DeepFM_3']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCN = DCN_BinClf_Torch.DCN_Layer(len(embedding_map_dict)+60,\n",
    "                                 20,\n",
    "                                 26,\n",
    "                                 13,\n",
    "                                 [1024 for _ in range(2)],\n",
    "                                 [0.5 for _ in range(3)],\n",
    "                                 6).to(DEVICE)\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'DCN_2')))\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:29:43 INFO   Model ROC Score: 0.819064\n"
     ]
    }
   ],
   "source": [
    "logger.info('Model ROC Score: {:.6f}'.format(execution.get_roc_auc_score_separate_inp(model, train_data, DEVICE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:30:42 INFO   Model ROC Score: 0.801416\n"
     ]
    }
   ],
   "source": [
    "logger.info('Model ROC Score: {:.6f}'.format(execution.get_roc_auc_score_separate_inp(model, test_data, DEVICE)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
